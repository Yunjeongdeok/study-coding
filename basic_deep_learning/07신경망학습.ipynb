{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyjyu4FzUAVw"
      },
      "source": [
        "# 신경망 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQvNez4qydhL"
      },
      "source": [
        "## 단순한 신경망 구현 : Logic Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7te43hqyiiJ"
      },
      "source": [
        "### 필요한 모듈 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Qf2F_YbdybBE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orUoPmDcymhj"
      },
      "source": [
        "### 하이퍼 파라미터(Hyper Parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "bOAmMxo0ymDF"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "lr = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjmLWgFVysnq"
      },
      "source": [
        "### 유틸 함수들(Util Functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Y4OMFGrjyq1c"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1/ (1+(np.exp(-x)))\n",
        "\n",
        "def mean_squared_error(pred_y, true_y):\n",
        "    return 0.5 * (np.sum(true_y - pred_y)**2)\n",
        "\n",
        "def cross_entropy_error(pred_y, true_y):\n",
        "    if true_y.ndim == 1:\n",
        "            true_y = true_y.reshape(1, -1)\n",
        "            pred_y = pred_y.reshape(1, -1)\n",
        "            \n",
        "    delta = 1e-7\n",
        "    batch_size = pred_y.shape[0]\n",
        "    return -np.sum(true_y * np.log(pred_y + delta))\n",
        "\n",
        "def cross_entropy_error_for_binary(pred_y, true_y):\n",
        "    return 0.5 * np.sum((-true_y * np.log(pred_y)) - (1 - true_y) * np.log(1-pred_y))\n",
        "\n",
        "def softmax(a):\n",
        "    exp_a = np.exp(a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "def differential(f, x):\n",
        "    eps = 1e-5\n",
        "    diff_value = np.zeros_like(x)\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        temp_val = x[i]\n",
        "\n",
        "        x[i] = temp_val + eps\n",
        "        f_h1 = f(x)\n",
        "\n",
        "        x[i] = temp_val -eps\n",
        "        f_h2 = f(x)\n",
        "\n",
        "        diff_value[i] = (f_h1 - f_h2) / (2 *eps)\n",
        "    \n",
        "    return diff_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Z2LTT_y3i5"
      },
      "source": [
        "### 신경망"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "gMTjjYgdy3D8"
      },
      "outputs": [],
      "source": [
        "class Logic_GateNet():\n",
        "    \n",
        "    def __init__(self):\n",
        "        def weight_init():\n",
        "            np.random.seed(1)\n",
        "            weights = np.random.randn(2)\n",
        "            bias = np.random.rand(1)\n",
        "\n",
        "            return weights, bias\n",
        "\n",
        "        self.weights, self.bias = weight_init()\n",
        "\n",
        "    def predict(self, x):\n",
        "        W = self.weights.reshape(-1, 1)\n",
        "        b = self.bias\n",
        "\n",
        "        pred_y = sigmoid(np.dot(x, W) + b)\n",
        "        return pred_y\n",
        "\n",
        "    def loss(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        return cross_entropy_error_for_binary(pred_y, true_y)\n",
        "\n",
        "    def get_gradient(self, x, t):\n",
        "        def loss_grad(grad):\n",
        "            return self.loss(x,t)\n",
        "\n",
        "        grad_W = differential(loss_grad, self.weights)\n",
        "        grad_B = differential(loss_grad, self.bias)\n",
        "\n",
        "        return grad_W, grad_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbNDoH_3zbGZ"
      },
      "source": [
        "### AND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P-ib8_RzHTh"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "rRiaACA6zGom"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100, Cost: 0.6886987628481974, Weights: [1.56402489 0.79138771], Bias: [-2.14869146]\n",
            "Epoch: 200, Cost: 0.4946922802317477, Weights: [2.01325161 1.71200974], Bias: [-3.07879437]\n",
            "Epoch: 300, Cost: 0.3920743574668242, Weights: [2.42795372 2.29704769], Bias: [-3.79078865]\n",
            "Epoch: 400, Cost: 0.32578054730407047, Weights: [2.79428901 2.73178113], Bias: [-4.37224115]\n",
            "Epoch: 500, Cost: 0.2786958490593001, Weights: [3.11570431 3.08342195], Bias: [-4.86530315]\n",
            "Epoch: 600, Cost: 0.24334520782392294, Weights: [3.39940551 3.38161091], Bias: [-5.29385366]\n",
            "Epoch: 700, Cost: 0.21578560009051095, Weights: [3.65216893 3.64181093], Bias: [-5.67294341]\n",
            "Epoch: 800, Cost: 0.1936925965478115, Weights: [3.87951811 3.87320526], Bias: [-6.01277885]\n",
            "Epoch: 900, Cost: 0.1755921064307146, Weights: [4.08579293 4.08179208], Bias: [-6.32065064]\n",
            "Epoch: 1000, Cost: 0.16049901156404384, Weights: [4.27438859 4.27176665], Bias: [-6.60197109]\n"
          ]
        }
      ],
      "source": [
        "AND = Logic_GateNet()\n",
        "\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "Y = np.array([[0],[0],[0],[1]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = AND.get_gradient(X,Y)\n",
        "\n",
        "    AND.weights -= lr * grad_W\n",
        "    AND.bias -= lr * grad_B\n",
        "\n",
        "    loss = AND.loss(X, Y)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, AND.weights, AND.bias))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZoyQv_czT7R"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-7CvWgc9zREa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00135585]\n",
            " [0.08865214]\n",
            " [0.08886421]\n",
            " [0.8748111 ]]\n"
          ]
        }
      ],
      "source": [
        "print(AND.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMXNiXWzts-"
      },
      "source": [
        "### OR Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ79pc4jzw3O"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8gnLmAyQzuoL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100, Cost: 0.4958958049609248, Weights: [2.45419153 1.40520415], Bias: [-0.14450602]\n",
            "Epoch: 200, Cost: 0.3399910052303018, Weights: [2.98517793 2.39365315], Bias: [-0.67661248]\n",
            "Epoch: 300, Cost: 0.2574778683612523, Weights: [3.44860944 3.08310291], Bias: [-1.03712038]\n",
            "Epoch: 400, Cost: 0.20645212636378812, Weights: [3.85036309 3.60706913], Bias: [-1.30581039]\n",
            "Epoch: 500, Cost: 0.17181024582382054, Weights: [4.19965722 4.02803989], Bias: [-1.5203541]\n",
            "Epoch: 600, Cost: 0.14680779380912212, Weights: [4.50602003 4.37937212], Bias: [-1.69913028]\n",
            "Epoch: 700, Cost: 0.1279564839577935, Weights: [4.77748492 4.68063256], Bias: [-1.85239584]\n",
            "Epoch: 800, Cost: 0.11326452414385635, Weights: [5.02041415 4.94419703], Bias: [-1.9864936]\n",
            "Epoch: 900, Cost: 0.10151135599420945, Weights: [5.23976713 5.17836955], Bias: [-2.10564229]\n",
            "Epoch: 1000, Cost: 0.09190796448458385, Weights: [5.43941599 5.38898714], Bias: [-2.21280016]\n"
          ]
        }
      ],
      "source": [
        "OR = Logic_GateNet()\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y_2 = np.array([[0],[1],[1],[1]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = OR.get_gradient(X, Y_2)\n",
        "\n",
        "    OR.weights -= lr * grad_W\n",
        "    OR.bias -= lr * grad_B\n",
        "\n",
        "    loss = OR.loss(X, Y_2)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, OR.weights, OR.bias))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWmEtX_VnLSI"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "JwPpOs3-z2vU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0986069 ]\n",
            " [0.95992825]\n",
            " [0.96182368]\n",
            " [0.99981878]]\n"
          ]
        }
      ],
      "source": [
        "print(OR.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEBhczCIz57Q"
      },
      "source": [
        "### NAND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzQaaHKKz8sZ"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "h463QUQRz8PS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100, Cost: 0.7911124259396919, Weights: [-0.49003662 -1.25822733], Bias: [1.74569496]\n",
            "Epoch: 200, Cost: 0.5429873559716376, Weights: [-1.51585766 -1.80296112], Bias: [2.79167048]\n",
            "Epoch: 300, Cost: 0.4211974656990432, Weights: [-2.14663639 -2.26687089], Bias: [3.56530812]\n",
            "Epoch: 400, Cost: 0.3455498971544839, Weights: [-2.60790032 -2.66357416], Bias: [4.18554164]\n",
            "Epoch: 500, Cost: 0.29306800971562547, Weights: [-2.97762276 -3.00565182], Bias: [4.70569391]\n",
            "Epoch: 600, Cost: 0.25427794229993145, Weights: [-3.28924936 -3.30437381], Bias: [5.1544371]\n",
            "Epoch: 700, Cost: 0.2243776808459012, Weights: [-3.55994902 -3.56859574], Bias: [5.54924485]\n",
            "Epoch: 800, Cost: 0.2006150644393949, Weights: [-3.79981845 -3.8050078 ], Bias: [5.90170075]\n",
            "Epoch: 900, Cost: 0.18128040519830899, Weights: [-4.01540776 -4.01865291], Bias: [6.21994667]\n",
            "Epoch: 1000, Cost: 0.16524895980992177, Weights: [-4.21127349 -4.21337527], Bias: [6.50995438]\n"
          ]
        }
      ],
      "source": [
        "NAND = Logic_GateNet()\n",
        "\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "Y_3 = np.array([[1],[1],[1],[0]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = NAND.get_gradient(X,Y_3)\n",
        "\n",
        "    NAND.weights -= lr * grad_W\n",
        "    NAND.bias -= lr * grad_B\n",
        "\n",
        "    loss = NAND.loss(X, Y_3)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, NAND.weights, NAND.bias))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR-rHaTU0Mga"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "WpzKW6sm0Ghp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.99851366]\n",
            " [0.90859333]\n",
            " [0.90876773]\n",
            " [0.12845438]]\n"
          ]
        }
      ],
      "source": [
        "print(NAND.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiTWfSQ60Zl2"
      },
      "source": [
        "### XOR Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmmL0VIu0bXq"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "0CGm0r1M0a9M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100, Cost: 1.4026786384666137, Weights: [ 0.46985501 -0.19958026], Bias: [-0.16093055]\n",
            "Epoch: 200, Cost: 1.3879402830296645, Weights: [ 0.15692157 -0.03421502], Bias: [-0.07306565]\n",
            "Epoch: 300, Cost: 1.38648983866543, Weights: [0.05486233 0.0005171 ], Bias: [-0.03311222]\n",
            "Epoch: 400, Cost: 1.3863225886766086, Weights: [0.02009022 0.00464887], Bias: [-0.01493932]\n",
            "Epoch: 500, Cost: 1.3862990114902494, Weights: [0.00759143 0.00320927], Bias: [-0.00667283]\n",
            "Epoch: 600, Cost: 1.3862951472088905, Weights: [0.00284948 0.00161098], Bias: [-0.00291262]\n",
            "Epoch: 700, Cost: 1.3862944853452563, Weights: [0.00096069 0.00061579], Bias: [-0.00120221]\n",
            "Epoch: 800, Cost: 1.3862943848646831, Weights: [1.77758040e-04 8.68769146e-05], Bias: [-0.0004242]\n",
            "Epoch: 900, Cost: 1.3862943785413158, Weights: [-0.0001567  -0.00017538], Bias: [-7.02997741e-05]\n",
            "Epoch: 1000, Cost: 1.386294383640712, Weights: [-0.00030268 -0.00030083], Bias: [9.06769007e-05]\n"
          ]
        }
      ],
      "source": [
        "XOR = Logic_GateNet()\n",
        "\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "Y_4 = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = XOR.get_gradient(X,Y_4)\n",
        "\n",
        "    XOR.weights -= lr * grad_W\n",
        "    XOR.bias -= lr * grad_B\n",
        "\n",
        "    loss = XOR.loss(X, Y_4)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, XOR.weights, XOR.bias))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy-ktElI0o5P"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "GWAJAJ_T0oqm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.50002267]\n",
            " [0.49994746]\n",
            " [0.499947  ]\n",
            " [0.49987179]]\n"
          ]
        }
      ],
      "source": [
        "print(XOR.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAlq_-6E1nIq"
      },
      "source": [
        "#### 2층 신경망으로 XOR 게이트 구현(1)\n",
        "\n",
        "- 얕은 신경망, Shallow Neural Network\n",
        "\n",
        "- 두 논리게이트(NAND, OR)를 통과하고  \n",
        "  AND 게이트로 합쳐서 구현\n",
        "\n",
        "- 06 신경망 구조 참고"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "mr7nYMG20jTo"
      },
      "outputs": [],
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "Y_5 = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "s1 = NAND.predict(X)\n",
        "s2 = OR.predict(X)\n",
        "X_2 = np.array([s1, s2]).T.reshape(-1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkTDx8Ah1xHY"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "LK2iD5A91yWQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.1286767 ]\n",
            " [0.79936003]\n",
            " [0.80077444]\n",
            " [0.14406688]]\n"
          ]
        }
      ],
      "source": [
        "print(AND.predict(X_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-SK4G262Agn"
      },
      "source": [
        "#### 2층 신경망으로 XOR 게이트 구현(2)\n",
        "- 클래스로 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "8RpnHCRZ1zwr"
      },
      "outputs": [],
      "source": [
        "class XORNet():\n",
        "    \n",
        "    def __init__(self):\n",
        "        np.random.seed(1)\n",
        "\n",
        "        def weight_init():\n",
        "            params = {}\n",
        "            params['w_1'] = np.random.randn(2)\n",
        "            params['b_1'] = np.random.rand(2)\n",
        "            params['w_2'] = np.random.randn(2)\n",
        "            params['b_2'] = np.random.rand(2)\n",
        "            return params\n",
        "        \n",
        "        self.params = weight_init()\n",
        "\n",
        "    def predict(self, x):\n",
        "        W_1, W_2 = self.params['w_1'].reshape(-1, 1), self.params['w_2'].reshape(-1, 1)\n",
        "        B_1, B_2 = self.params['b_1'], self.params['b_2']\n",
        "        \n",
        "        A1 = np.dot(x, W_1) + B_1\n",
        "        Z1 = sigmoid(A1)\n",
        "        A2 = np.dot(Z1,W_2) + B_2\n",
        "        pred_y = sigmoid(A2)\n",
        "\n",
        "        return pred_y\n",
        "\n",
        "    def loss(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        return cross_entropy_error_for_binary(pred_y, true_y)\n",
        "\n",
        "    def get_gradient(self, x, t):\n",
        "        def loss_grad(grad):\n",
        "            return self.loss(x, t)\n",
        "\n",
        "        grads ={}\n",
        "        grads['w_1'] = differential(loss_grad, self.params['w_1'])\n",
        "        grads['b_1'] = differential(loss_grad, self.params['b_1'])\n",
        "        grads['w_2'] = differential(loss_grad, self.params['w_2'])\n",
        "        grads['b_2'] = differential(loss_grad, self.params['b_2'])\n",
        "        \n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lplK_x0l2YLh"
      },
      "source": [
        "#### 하이퍼 파라미터(Hyper Parameter)\n",
        "- 재조정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "qf-3wWSv2b7l"
      },
      "outputs": [],
      "source": [
        "lr = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmHKd45d2JbJ"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "cQNd3XVd2Gj7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 100, Cost: 2.585909640501895\n",
            "Epoch: 100, Cost: 2.5852074864235943\n",
            "Epoch: 100, Cost: 2.5836759733080656\n",
            "Epoch: 100, Cost: 2.5833633336169957\n",
            "Epoch: 200, Cost: 0.6588289820436902\n",
            "Epoch: 200, Cost: 0.6583203735170571\n",
            "Epoch: 200, Cost: 0.652390680536987\n",
            "Epoch: 200, Cost: 0.6517074396761213\n",
            "Epoch: 300, Cost: 0.25183276924404263\n",
            "Epoch: 300, Cost: 0.25177064238891833\n",
            "Epoch: 300, Cost: 0.25068822482319797\n",
            "Epoch: 300, Cost: 0.2504602632346608\n",
            "Epoch: 400, Cost: 0.15018155190482163\n",
            "Epoch: 400, Cost: 0.1501618047310399\n",
            "Epoch: 400, Cost: 0.1497489243044297\n",
            "Epoch: 400, Cost: 0.14965075200907407\n",
            "Epoch: 500, Cost: 0.10599776462524572\n",
            "Epoch: 500, Cost: 0.10598869467701219\n",
            "Epoch: 500, Cost: 0.10577532840309056\n",
            "Epoch: 500, Cost: 0.10572230825999189\n",
            "Epoch: 600, Cost: 0.08159467276802451\n",
            "Epoch: 600, Cost: 0.08158962653482503\n",
            "Epoch: 600, Cost: 0.0814602261671057\n",
            "Epoch: 600, Cost: 0.08142737846397785\n",
            "Epoch: 700, Cost: 0.06620099755966835\n",
            "Epoch: 700, Cost: 0.06619784171364412\n",
            "Epoch: 700, Cost: 0.06611126553607412\n",
            "Epoch: 700, Cost: 0.06608901862737993\n",
            "Epoch: 800, Cost: 0.05563454415945304\n",
            "Epoch: 800, Cost: 0.055632409268933695\n",
            "Epoch: 800, Cost: 0.05557051814464002\n",
            "Epoch: 800, Cost: 0.055554488757384266\n",
            "Epoch: 900, Cost: 0.04794518499262157\n",
            "Epoch: 900, Cost: 0.047943657128352093\n",
            "Epoch: 900, Cost: 0.047897254490206376\n",
            "Epoch: 900, Cost: 0.047885169684664323\n",
            "Epoch: 1000, Cost: 0.04210484205238309\n",
            "Epoch: 1000, Cost: 0.04210370138980961\n",
            "Epoch: 1000, Cost: 0.04206764081681853\n",
            "Epoch: 1000, Cost: 0.04205820993883537\n"
          ]
        }
      ],
      "source": [
        "XOR = XORNet()\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "Y_5 = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grads =XOR.get_gradient(X, Y_5)\n",
        "\n",
        "    for key in ('w_1', 'b_1', 'w_2', 'b_2'):\n",
        "        XOR.params[key] -= lr * grads[key]\n",
        "\n",
        "        loss = XOR.loss(X, Y_5)\n",
        "        train_loss_list.append(loss)\n",
        "\n",
        "        if i % 100 == 99:\n",
        "            print(\"Epoch: {}, Cost: {}\".format(i+1, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIV_GsoG2eDs"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "Dpr0nZhc2Szr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00845742 0.00845742]\n",
            " [0.98352814 0.98352814]\n",
            " [0.99160907 0.99160907]\n",
            " [0.00849318 0.00849318]]\n"
          ]
        }
      ],
      "source": [
        "print(XOR.predict(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1IuDL8R7wrx"
      },
      "source": [
        "## 다중 클래스 분류 : MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CiJ5Gmq9Wpa"
      },
      "source": [
        "### 배치 처리\n",
        "- 학습 데이터 전체를 한번에 진행하지 않고  \n",
        "  일부 데이터(샘플)을 확률적으로 구해서 조금씩 나누어 진행\n",
        "\n",
        "- 확률적 경사 하강법(Stochastic Gradient Descent) 또는  \n",
        "  미니 배치 학습법(mini-batch learning)이라고도 부름"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUDNWwj49byH"
      },
      "source": [
        "#### 신경망 구현 : MNIST "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjBRQYlP74GM"
      },
      "source": [
        "#### 필요한 모듈 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "h0lJbkuW71lm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDvtEiD77_gu"
      },
      "source": [
        "#### 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "4WL7zXMl_uo9"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_rNg5Jn8FRA"
      },
      "source": [
        "#### 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "u4wpsQGA8BOO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "pU7nvkHO8IFR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(28, 28)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img = x_train[0]\n",
        "print(img.shape)\n",
        "\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "WbBA1Kl18KGT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTFu8i-z8U_C"
      },
      "source": [
        "#### 데이터 전처리 (Data Preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "q76pjKDVftHJ"
      },
      "outputs": [],
      "source": [
        "def flatten_for_mnist(x):\n",
        "    temp = np.zeros((x.shape[0], x[0].size))\n",
        "\n",
        "    for idx, data in enumerate(x):\n",
        "        temp[idx, :] = data.flatten()\n",
        "        \n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "vvMWrDOR8Mns"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "(60000, 10)\n",
            "(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = flatten_for_mnist(x_train)\n",
        "x_test = flatten_for_mnist(x_test)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train_ohe = tf.one_hot(y_train, depth=10).numpy()\n",
        "y_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n",
        "\n",
        "print(y_train_ohe.shape)\n",
        "print(y_test_ohe.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "9LjpWz0dotJs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0 0.0\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(x_train[0].max(), x_train[0].min())\n",
        "print(y_train_ohe[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GUaa92Y9RhY"
      },
      "source": [
        "#### 하이퍼 파라미터(Hyper Parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "sk3FXXLi9Th5"
      },
      "outputs": [],
      "source": [
        "epochs = 2\n",
        "lr = 0.1\n",
        "batch_size = 100\n",
        "train_size = x_train.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lMJ0h8p8iZl"
      },
      "source": [
        "#### 사용되는 함수들(Util Functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "bSlqZ2Xx8hFn"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1/ (1+(np.exp(-x)))\n",
        "\n",
        "def mean_squared_error(pred_y, true_y):\n",
        "    return 0.5 * (np.sum(true_y - pred_y)**2)\n",
        "\n",
        "def cross_entropy_error(pred_y, true_y):\n",
        "    if true_y.ndim == 1:\n",
        "            true_y = true_y.reshape(1, -1)\n",
        "            pred_y = pred_y.reshape(1, -1)\n",
        "            \n",
        "    delta = 1e-7\n",
        "    batch_size = pred_y.shape[0]\n",
        "    return -np.sum(true_y * np.log(pred_y + delta))\n",
        "\n",
        "def cross_entropy_error_for_binary(pred_y, true_y):\n",
        "    return 0.5 * np.sum((-true_y * np.log(pred_y)) - (1 - true_y) * np.log(1-pred_y))\n",
        "\n",
        "def softmax(a):\n",
        "    exp_a = np.exp(a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "def differential_1d(f, x):\n",
        "    eps = 1e-5\n",
        "    diff_value = np.zeros_like(x)\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        temp_val = x[i]\n",
        "\n",
        "        x[i] = temp_val + eps\n",
        "        f_h1 = f(x)\n",
        "\n",
        "        x[i] = temp_val -eps\n",
        "        f_h2 = f(x)\n",
        "\n",
        "        diff_value[i] = (f_h1 - f_h2) / (2 *eps)\n",
        "    \n",
        "    return diff_value\n",
        "\n",
        "def differential_2d(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return differential_1d(f,X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "    \n",
        "    for idx, x in enumerate(X):\n",
        "        grad[idx] = differential_1d(f,x)\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSoV9fyj8_u7"
      },
      "source": [
        "#### 2층 신경망으로 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "XBObD5Fw89HI"
      },
      "outputs": [],
      "source": [
        "class MyModel():\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        def weight_init(input_nodes, hidden_nodes, output_units):\n",
        "            np.random.seed(777) \n",
        "            params = {}\n",
        "            params['w_1'] = 0.01 * np.random.randn(input_nodes, hidden_nodes)\n",
        "            params['b_1'] = np.random.rand(hidden_nodes)\n",
        "            params['w_2'] = 0.01 * np.random.randn(hidden_nodes, output_units)\n",
        "            params['b_2'] = np.zeros(output_units)\n",
        "            \n",
        "            return params\n",
        "            \n",
        "        self.params = weight_init(784, 64, 10)\n",
        "\n",
        "    def predict(self, x):\n",
        "        W_1, W_2 = self.params['w_1'], self.params['w_2']\n",
        "        B_1, B_2 = self.params['b_1'], self.params['b_2']\n",
        "        \n",
        "        A1 = np.dot(x, W_1) + B_1\n",
        "        Z1 = sigmoid(A1)\n",
        "        A2 = np.dot(Z1,W_2) + B_2\n",
        "        pred_y = softmax(A2)\n",
        "\n",
        "        return pred_y\n",
        "\n",
        "    def loss(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        return cross_entropy_error_for_binary(pred_y, true_y)\n",
        "\n",
        "    def accuracy(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        y_argmax = np.argmax(pred_y, axis=1)\n",
        "        t_argmax = np.argmax(true_y, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y_argmax == t_argmax) / float(x.shape[0])\n",
        "        return accuracy    \n",
        "    \n",
        "    \n",
        "    def get_gradient(self, x, t):\n",
        "        def loss_grad(grad):\n",
        "            return self.loss(x, t)\n",
        "\n",
        "        grads ={}\n",
        "        grads['w_1'] = differential_2d(loss_grad, self.params['w_1'])\n",
        "        grads['b_1'] = differential_2d(loss_grad, self.params['b_1'])\n",
        "        grads['w_2'] = differential_2d(loss_grad, self.params['w_2'])\n",
        "        grads['b_2'] = differential_2d(loss_grad, self.params['b_2'])\n",
        "        \n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maKNIlK-xJ5k"
      },
      "source": [
        "#### 모델 생성 및 학습\n",
        "- 시간 많이 소요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "XSEARgNIop8t"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "566bd22d850b4e3faff52fea441b493b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs: 1, Cost: 544.8502909912949, Train Accuracy: 0.0993, Test Accuray: 0.1032\n",
            "Epochs: 2, Cost: 359.9811386321985, Train Accuracy: 0.09751666666666667, Test Accuray: 0.0974\n",
            "총 학습 소요시간: 130.609s\n"
          ]
        }
      ],
      "source": [
        "model = MyModel()\n",
        "\n",
        "train_loss_list = list()\n",
        "train_acc_list = list()\n",
        "test_acc_list = list()\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "start_time = time.time()\n",
        "for i in tqdm(range(epochs)):\n",
        "    \n",
        "    batch_idx = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_idx]\n",
        "    y_batch = y_train_ohe[batch_idx]\n",
        "\n",
        "    grads = model.get_gradient(x_batch, y_batch)\n",
        "    \n",
        "    for key in grads.keys():\n",
        "        model.params[key] -= lr * grads[key]\n",
        "        \n",
        "    loss = model.loss(x_batch, y_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    train_accuracy = model.accuracy(x_train, y_train_ohe)\n",
        "    test_accuracy = model.accuracy(x_test, y_test_ohe)\n",
        "    train_acc_list.append(train_accuracy)\n",
        "    test_acc_list.append(test_accuracy)\n",
        "\n",
        "    print(\"Epochs: {}, Cost: {}, Train Accuracy: {}, Test Accuray: {}\".format(i+1, loss, train_accuracy, test_accuracy))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"총 학습 소요시간: {:.3f}s\".format(end_time - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7nL8f20x4zl"
      },
      "source": [
        "### 모델의 결과\n",
        "- 모델은 학습이 잘 될 수도, 잘 안될 수도 있음\n",
        "\n",
        "- 만약, 학습이 잘 되지 않았다면,  \n",
        "  학습이 잘 되기 위해서 어떠한 조치를 취해야 하는가?\n",
        "  - 다양한 학습관련 기술이 존재"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('study')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "9e5ba8102417ed24000ae30d0ee823b0df91ab90972766c536e2f5d51ee26514"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
